% Copyright (c) 2025 Carl Martin Ludvig Sinander.

% This program is free software: you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation, either version 3 of the License, or
% (at your option) any later version.

% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License for more details.

% You should have received a copy of the GNU General Public License
% along with this program. If not, see <https://www.gnu.org/licenses/>.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Dynamic programming is a way of characterising optimal choices, and the payoffs they generate, in single-agent dynamic decision problems. In this chapter and the next, we extend the logic of dynamic programming to characterise equilibria and equilibrium payoffs in dynamic games.

By a `single-agent decision problem', I mean an extensive-form game with only one player, where randomness is captured by moves of Nature and uncertainty is captured by information sets. `Dynamic programming' refers to a recursive way of viewing such problems which was suggested by \textcite{Bellman1952,Bellman1954,Bellman1957} and then developed by, among others, \textcite{Shapley1953,Karlin1955,Howard1960,Blackwell1962,Blackwell1965}. Its two key elements are Blackwell's \emph{one-shot deviation principle} and the \emph{Bellman equation.}

\begin{itemize}

	\item The one-shot deviation principle is a property of optimal \emph{choices:} it asserts that a strategy is optimal if (and, obviously, only if) at every information set, the agent cannot strictly improve her payoff by changing her action at that information set alone (leaving unchanged her actions at each of her other information sets). In other words, a strategy admits no strictly profitable deviations (i.e. is optimal) if and only if it admits no strictly profitable \emph{one-shot} deviations. In the special case of finite-horizon decision problems, (all and only) strategies which admit no strictly profitable one-shot deviations may be found using the backward-induction algorithm.

	\item The Bellman equation captures a property of optimal \emph{payoffs,} i.e. the payoffs generated by choosing optimally. The \emph{(optimal) value function} specifies, for each information set, the maximal available (continuation) payoff when starting from that information set; in other words, the (continuation) payoff that the agent obtains if she begins at this information set and behaves optimally going forward. The Bellman equation is a certain functional equation (that is, an equation in an unknown \emph{function,} rather than e.g. in an unknown real number). The value function solves the Bellman equation, and typically it is the unique solution within some natural class of functions (e.g. the class of all bounded functions). Thus the value function may be found by solving the Bellman equation; furthermore, doing so delivers an optimal strategy as a by-product. The Bellman equation is particularly useful and insightful in \emph{stationary} decision problems.

\end{itemize}

This chapter concerns the (conceptually straightforward) extension of the one-shot deviation principle to dynamic games. A very general result of this type could be proved for sequential equilibria of arbitrary perfect-recall extensive-form games \parencite[see e.g.][Exercise~227.1]{OsborneRubinstein1994}. We shall keep things simple by restricting attention to subgame-perfect equilibria of \emph{deterministic perfect-information} extensive-form games, i.e. those games in which there are no moves of Nature and every information set is a singleton. One application is to the \textcite{Rubinstein1982} bargaining model.

In the next chapter, we undertake the (conceptually more significant) extension of the Bellman equation to dynamic games. We focus on the game-theoretic analogue of stationary single-agent decision problems: \emph{repeated games.} Specifically, we prove a Bellman-equation-type characterisation of the set of perfect public equilibrium payoffs of repeated games with (possibly noisy) public monitoring, as well as a means of constructing, for any given equilibrium payoff, an equilibrium which generates it. This recursive technique is called `APS' after its originators, \textcite{AbreuPearceStacchetti1990}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some concepts and notation}
\label{osdp:recap}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recall the standard concepts from extensive-form game theory (tersely) reviewed in \cref{ch0:extensive} above, including the formal description of an extensive-form game in terms of histories and the notion of subgame-perfect equilibrium.

Fix a deterministic perfect-information extensive-form game
%
\begin{equation*}
	(I,\mathcal{A},H,P,A,\mathcal{S},(u_i)_{i \in I},\pi) ,
\end{equation*}
%
with terminal histories denoted $Z \subseteq H$, recalling that $I$ are the players, $\mathcal{A}$ the actions, $H$ the histories, $\mathcal{S}$ the information sets (all of which are singletons, by definition of `perfect information'), $(u_i)_{i \in I}$ the payoff functions, and the functions $P$, $A$ and $\pi$ specify, for each information set, which player (or Nature) moves, which actions are available and, if Nature moves, the probabilities with which the various actions are chosen.
Given the determinacy assumption, Nature never moves, so we may as well drop $\pi$ from the formal description of the game. Similarly, given the perfect-information assumption, we can drop the information sets $\mathcal{S}$ from the definition, and treat $P$, $A$, and strategy profiles as having domain $H \setminus Z$ rather than $\mathcal{S}$. That leaves $(I,\mathcal{A},H,P,A,(u_i)_{i \in I})$.

A \emph{strategy profile} is a map $\sigma : H \setminus Z \to \Delta(\mathcal{A})$ such that $\supp \sigma(h) \subseteq A(h)$ for every $h \in H \setminus Z$. We sometimes write $\sigma \equiv (\sigma_i)_{i \in I}$, where for each player $i \in I$, $\sigma_i$ denotes the restriction of $\sigma$ to those non-terminal histories $P^{-1}(i) \equiv \{ h \in H \setminus Z : P(h) = i \}$ at which player~$i$ moves,%
	\footnote{That is, $\sigma_i$ is the map $P^{-1}(i) \to \Delta(\mathcal{A})$ given by $\sigma_i(h) \coloneqq \sigma(h)$ for every $h \in P^{-1}(i)$.}
and is called a \emph{strategy of player~$i$.}

Any non-terminal history $h \in H \setminus Z$ corresponds to a subgame, viz. the subgame starting immediately after $h$ has been played. The histories of this subgame are $H|_h \coloneqq \{ h' \in H : \text{$h$ is a truncation of $h'$} \}$, and its non-terminal histories are $H|_h \setminus Z$. For any strategy profile $\sigma$ and any non-terminal history $h \in H \setminus Z$ (even if $h$ is off the path of $\sigma$), the \emph{continuation strategy} is the restriction of $\sigma$ to $H|_h \setminus Z$, denoted $\sigma|_h$.%
	\footnote{That is, $\sigma|_h$ is the function $H|_h \setminus Z \to \Delta(\mathcal{A})$ defined by $\sigma|_h(h') \coloneqq \sigma(h')$ for every $h' \in H|_h$.}
The continuation strategy is a strategy of the subgame starting at $h$, and describes how the strategy $\sigma$ plays if and when this subgame is reached.

Payoffs $u(z) \coloneqq (u_i(z))_{i \in I}$ depend on which \emph{terminal} history $z \in Z$ is reached. For any \emph{non}-terminal history $h \in H \setminus Z$ and pure strategy profile $\sigma$, letting $z$ be the terminal history reached when players first play $h$ and then follow $\sigma|_h$,%
	\footnote{That is, $z \coloneqq ( h, \sigma(h), \sigma( h, \sigma(h) ), \sigma( h, \sigma(h), \sigma( h, \sigma(h) ) ), \dots )$.}
we abuse notation by writing $u(h,\sigma|_h) \equiv u(z)$ for the resulting payoffs. When $\sigma$ is a behavioural strategy profile, $h$ and $\sigma|_h$ generate a probability distribution over terminal nodes $z$, and we write $u(h,\sigma|_h)$ for the expectation of $u(z)$ with respect to this distribution.

A strategy profile $\sigma = (\sigma_i)_{i \in I}$ is \emph{subgame-perfect} if and only if for every non-terminal history $h \in H \setminus Z$, letting $i \coloneqq P(h)$ be the player who moves at $h$, it holds that $u_i(h,\sigma|_h) \geq u_i(h,(\sigma'_i,\sigma_{-i})|_h)$ for every strategy $\sigma'_i$ of player~$i$. This is equivalent to the usual definition, which is that $\sigma$ is subgame-perfect if and only if for every non-terminal history $h \in H \setminus Z$, the continuation strategy profile $\sigma|_h$ is a Nash equilibrium of the subgame starting at $h$.

\begin{example}
	%
	\label{example:rubinstein}
	%
	Consider the \textcite{Rubinstein1982} model of bargaining.%
		\footnote{Also called the `Rubinstein--St√•hl model' after \textcite{Stahl1972,Stahl1977}, who studied a version with finite horizon and discretised action spaces.}
	Player~1 and player~2 seek to split a unit surplus. We shall formalise a split as a number $s \in [0,1]$, meaning that player~1 gets share $s$ while player~2 gets $1-s$. In each period $t \in \N$, provided an agreement has not already been reached, one player makes a proposal $s \in [0,1]$, and the other player observes this proposal and then either accepts it or rejects it. If the proposal accepted, then the game ends in period $t$, with the surplus being split according to the accepted proposal, generating (total, lifetime) payoffs of $\delta_1^{t-1} s$ for player~1 and $\delta_2^{t-1} (1-s)$ for player~2, where $\delta_1,\delta_2 \in (0,1)$ are the players' discount factors. If the proposal is instead rejected, then period $t+1$ dawns. The proposer in period $t$ is player~1 in case $t$ is odd and player~2 in case $t$ is even. If no proposal is ever accepted, then both players' payoffs are zero.

	This is a deterministic perfect-information extensive-form game:
	%
	\begin{itemize}
	
		\item the players are $I \coloneqq \{1,2\}$,

		\item the actions are $\mathcal{A} \coloneqq [0,1] \cup \{\text{accept},\text{reject}\}$

		\item the non-terminal histories $H \setminus Z$ are all finite sequences in $\mathcal{A}$ (including the empty sequence) whose every odd entry belongs to $[0,1]$ and whose every even entry equals `reject',

		\item the terminal histories $Z$ are all sequences in $\mathcal{A}$ (not including the empty sequence) of even or infinite length whose every odd entry belongs to $[0,1]$, whose every even entry except the last (if there is a last entry) equals `reject', and whose last entry (if there is one) equals `accept',

		\item the player function $P$ is given by, for each $n \in \{0,1,2,3,\dots\}$, 
		%
		\begin{equation*}
			P(h)
			\coloneqq
			\begin{cases}
				1 & \text{for $h$ of length $4n$ or $4n+3$} \\
				2 & \text{for $h$ of length $4n+1$ or $4n+2$,}
			\end{cases} 
		\end{equation*}

		\item the action function $A$ is given by $A(h) \coloneqq [0,1]$ for $h$ of even length and $A(h) \coloneqq \{ \text{accept}, \text{reject} \}$ for $h$ of odd length, and

		\item the payoff functions $(u_1,u_2)$ are given by
		%
		\begin{equation*}
			(u_1(z),u_2(z)) \coloneqq \left( \delta_1^{t-1} s_t, \delta_2^{t-1} (1-s_t) \right)
		\end{equation*}
		%
		for $z = (s_1,\text{reject},s_2,\text{reject},s_3,\text{reject},\dots,s_t,\text{accept})$ and
		%
		\begin{equation*}
			(u_1(z),u_2(z)) \coloneqq (0,0)
		\end{equation*}
		%
		for any $z \in Z$ of infinite length.
	
	\end{itemize}
	%
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The OSD principle}
\label{osdp:principle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Fix a deterministic perfect-information extensive-form game
%
\begin{equation*}
	\mathcal{G}
	= (I,\mathcal{A},H,P,A,(u_i)_{i \in I}) .
\end{equation*}
%
Given a strategy profile $\sigma$, we say that another strategy profile $\sigma'$ is a \emph{one-shot deviation (from $\sigma$)} iff $\sigma$ and $\sigma'$ differ at exactly one non-terminal history: that is, there is a non-terminal history $h \in H \setminus Z$ such that $\sigma(h) \neq \sigma'(h)$ and $\sigma(h') = \sigma'(h')$ for every $h' \in (H \setminus Z) \setminus \{h\}$. A strategy profile $\sigma$ is called \emph{one-shot unimprovable} iff there is no one-shot deviation $\sigma'$ from $\sigma$ that makes the deviating player strictly better off. Formally:

\begin{definition}
	%
	\label{definition:oneshot_unimp}
	%
	A strategy profile $\sigma$ is \emph{one-shot unimprovable} iff for any one-shot deviation $\sigma'$ from $\sigma$, letting $h \in H \setminus Z$ be the unique non-terminal history at which $\sigma(h) \neq \sigma'(h)$ and writing $i \coloneqq P(h)$ for the player who moves at that history, it holds that $u_i(h,\sigma|_h) \geq u_i(h,(\sigma'_i,\sigma_{-i})|_h)$.%
	\footnote{Note that the non-terminal history $h \in H \setminus Z$ may be off the path of $\sigma$. In other words, one-shot unimprovability demands immunity to one-shot deviations both on- and off-path.}
	%
\end{definition}

Obviously subgame-perfection implies one-shot unimprovability. (Make sure that you understand why.) The converse implication holds in some games and not others, and has a name:

\begin{definition}
	%
	\label{definition:osdp}
	%
	A deterministic perfect-information extensive-form game satisfies the \emph{one-shot deviation principle} (`OSD principle' for short) iff every one-shot unimprovable strategy profile is subgame-perfect. 
	%
\end{definition}

In games that satisfy it, the OSD principle is a very useful property, allowing subgame-perfection to be verified by checking only simple one-shot deviations, rather than complicated arbitrary deviations (which may involve simultaneously changing the actions taken at many histories, even infinitely many).

\addtocounter{example}{-1}
\begin{example}[continued]
	%
	\label{example:rubinstein_osd}
	%
	Define
	%
	\begin{equation*}
		x \coloneqq
		\frac{ 1 - \delta_2 }{ 1 - \delta_1 \delta_2 }
		\quad \text{and} \quad
		y \coloneqq \delta_1 x .
	\end{equation*}
	%
	Let $\sigma$ be the strategy profile according to which when proposing (whatever the history), player~1 proposes $x$ and player~2 proposes $y$, and when contemplating a proposal $s \in [0,1]$ (whatever the prior history), player~1 accepts iff $s \geq y$ and player~2 accepts iff $s \leq x$.

	Consider the strategy profile $\sigma'$ that agrees with $\sigma$ at every non-terminal history, except that $\sigma'(h) \coloneqq \text{reject}$ (whereas $\sigma(h) = \text{accept}$) at a single history $h$ in which the last action taken was a proposal of $y$ by player~2. Evidently $\sigma'$ is a one-shot deviation from $\sigma$. Since $y = \delta_1 x$, player~1 is indifferent at the history $h$ between sticking to $\sigma$ and (one-shot) deviating to $\sigma'$: writing $t \coloneqq [\text{length}(h)+1]/2$ for the period in which this takes place, we have
	%
	\begin{equation*}
		u_1(h,\sigma|_h)
		= \delta_1^{t-1} y
		= \delta_1^t x
		= u_1(h,\sigma'|_h) ,
	\end{equation*}
	%
	where the final equality holds since under the one-shot deviation $\sigma'$, what player~1 does after rejecting player~2's proposal of $y$ is to revert to $\sigma$, which means proposing $x$, whereupon player~2 accepts.

	Symmetrically, because $x$ and $y$ satisfy $1-x = \delta_2 (1-y)$, player~2 is indifferent, when contemplating a proposal of $x$, between accepting (as $\sigma$ dictates) and one-shot deviating by rejecting the proposal and then reverting to $\sigma$. Using these two indifference conditions, it is easy to show that $\sigma$ is one-shot unimprovable; the argument is left as an exercise (\Cref{exercise:rubinstein_osd} below).

	But is $\sigma$ subgame-perfect? To show directly that it is, we would have to show that at every non-terminal history $h \in H \setminus Z$, letting $i \coloneqq P(h)$ be the player who moves, there exists no strategy $\sigma_i'$ of player~$i$ such that $u_i(h,(\sigma_i',\sigma_{-i})_h) > u_i(h,\sigma|_h)$. There are very many such alternative strategies $\sigma_i'$, since we must consider those which differ from $\sigma_i$ at many (even infinitely many) histories. This makes checking subgame-perfection hard.

	This headache goes away if we can show that the Rubinstein model satisfies the one-shot deviation principle: in that case, since $\sigma$ is one-shot unimprovable, we may conclude that it is subgame-perfect.
	%
\end{example}

\begin{exercise}
	%
	\label{exercise:rubinstein_osd}
	%
	Carefully prove that the strategy profile $\sigma$ in \Cref{example:rubinstein_osd} is one-shot unimprovable.
	%
\end{exercise}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backward induction}
\label{osdp:backward}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A deterministic perfect-information extensive-form game
%
\begin{equation*}
	\mathcal{G}
	= (I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
\end{equation*}
%
has \emph{finite horizon} iff every $h \in H$ has finite length. Recall that \emph{backward induction} is an algorithm which takes as input a finite-horizon deterministic extensive-form game and outputs a set of pure strategy profiles. 

\begin{itemize}

	\item The outputted set of strategy profiles has $\geq 1$ elements (i.e. is non-empty) whenever the inputted game $\mathcal{G}$ has finite horizon and a finite action set $\mathcal{A}$. More generally, non-emptiness can be guaranteed by continuity-and-compactness hypotheses on the payoffs $(u_i)_{i \in I}$ and action sets $\{ A(h) : h \in H \setminus Z \}$.

	\item The outputted set of strategy profiles has $\leq 1$ elements except when the inputted game $\mathcal{G}$ features payoff ties, i.e. when there exist strategy profiles $\sigma$ and $\sigma'$ such that $\sigma'$ is a one-shot deviation from $\sigma$ with no payoff consequences for the deviator: $u_i(h,\sigma|_h) = u_i(h,\sigma'|_h)$, where $h$ denotes the unique history at which $\sigma(h) \neq \sigma(h')$ and $i \coloneqq P(h)$ is the player who moves at that history.

\end{itemize}

What the backward-induction algorithm does is precisely to rule out one-shot deviations that make the deviating player strictly better off: the action that the algorithm specifies at each non-terminal history $h \in H \setminus Z$ is determined by considering the payoff consequences (for the moving player $P(h)$) of choosing alternative actions $\in A(h)$ at $h$, while holding fixed the actions taken at other histories (in particular, at successor histories, i.e. those histories $h'$ of which $h$ is a truncation). To really spell it out, the backward-induction algorithm proceeds as follows:

\begin{enumerate}

	\item \label{item:backward_induction:1} Actions are specified at each of the final (=maximal in the sense of set inclusion $\subseteq$) non-terminal histories, in such a way that at each of these histories, there is no one-shot deviation that makes the deviating player strictly better off.

	\item \label{item:backward_induction:2} Actions are specified at each of the penultimate non-terminal histories, in such a way that at each of these histories, there is no one-shot deviation that makes the deviating player strictly better off, given play at the final non-terminal histories.

	\item \label{item:backward_induction:3} Actions are specified at each of the antepenultimate non-terminal histories, in such a way that at each of these histories, there is no one-shot deviation that makes the deviating player strictly better off, given play at the penultimate and final non-terminal histories.

	\item[\vdots]

	\item[($T$)] Actions are specified at the initial history (the empty history $\varnothing$), in such a way that at this history, there is no one-shot deviation that makes the deviating player strictly better off, given play at all of the subsequent non-terminal histories.

\end{enumerate}

Recall the familiar relationship between the backward-induction algorithm and subgame-perfection:

\begin{fact}
	%
	\label{fact:bi_spne}
	%
	For any deterministic finite-horizon perfect-information extensive-form game, if a strategy profile is outputted by the backward-induction algorithm, then it is subgame-perfect.
	%
\end{fact}

Since what the backward-induction algorithm does is precisely to rule out one-shot deviations that make the deviating player strictly better off, the familiar \Cref{fact:bi_spne} may equivalently rephrased as:

\begin{proposition}
	%
	\label{proposition:osdp_finite}
	%
	Every deterministic finite-horizon perfect-information exten\-sive-form game satisfies the one-shot deviation principle.
	%
\end{proposition}

The remaining question is which infinite-horizon games (i.e. games in which some histories $h \in H$ have infinite length) satisfy the OSD principle.
There is obviously no analogue of the backward-induction algorithm for such games, so the OSD principle, if valid, is especially analytically valuable. And infinite-horizon games abound in economics. (For good reason: models with a finite, deterministic and commonly known length tend to exhibit strong `deadline effects' which one often wishes to abstract from, because they are a distraction and/or are economically implausible.)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{How the OSD principle can fail}
\label{osdp:invalid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Not all infinite-horizon games satisfy the OSD principle.

\begin{exercise}
	%
	\label{exercise:marathon}
	%
	Consider Jeff Ely's (\citeyear{Ely2017online}) Marathon game. There is a single player called Zeno, two actions called `continue' and `quit', and infinitely many periods $t \in \{0,1,2,3,\dots\}$. In each period, Zeno is called upon to act if and only if he has never previously chosen `quit'. Whenever he is called upon to act, his choice is between `continue' and `quit'. Zeno's (total, lifetime) payoff is $-t/(t+1)$ in case he quits in period $t \in \{0,1,2,3,\dots\}$, and $1$ in case he never quits. He has perfect recall.%
		\footnote{Formally:
		players $I \coloneqq \{\text{Zeno}\}$,
		actions $\mathcal{A} \coloneqq \{ \text{c(ontinue),q(uit)} \}$,
		non-terminal histories $H \setminus Z \coloneqq \left\{ \varnothing, (\text{c}), (\text{c},\text{c}), (\text{c},\text{c},\text{c}), (\text{c},\text{c},\text{c},\text{c}), (\text{c},\text{c},\text{c},\text{c},\text{c}), \dots \right\}$, 
		terminal histories $Z \coloneqq \left\{ (\text{q}), (\text{c},\text{q}), (\text{c},\text{c},\text{q}), (\text{c},\text{c},\text{c},\text{q}), (\text{c},\text{c},\text{c},\text{c},\text{q}), \dots \right\} \cup \left\{ (\text{c},\text{c},\text{c},\text{c},\text{c},\dots) \right\}$,
		player and action functions given by $P(h) \coloneqq \text{Zeno}$ and $A(h) \coloneqq \{ \text{c}, \text{q} \}$ for every $h \in H \setminus Z$,
		information sets $\mathcal{S} \coloneqq \{ \{h\} : h \in H \setminus Z \}$,
		and payoff function given by $u_{\text{Zeno}}(z) \coloneqq -t/(t+1)$ for $z = (\underbrace{\text{c},\dots,\text{c}}_{\text{$t$ times}},\text{q})$ and $u_{\text{Zeno}}((\text{c},\text{c},\text{c},\text{c},\text{c},\dots)) \coloneqq 1$.}

	\begin{enumerate}[label=(\alph*)]

		\item Show that `always continue' is the unique subgame-perfect strategy.

		\item Show that the strategy `always continue' is one-shot unimprovable.

		\item Show that the strategy `always quit' is one-shot unimprovable.

		\item Does the Marathon game satisfy the one-shot deviation principle?

		\item Bonus: show that there are no one-shot unimprovable strategies besides `always continue' and `always quit'.
	
	\end{enumerate}
	%
\end{exercise}

\begin{exercise}
	%
	\label{exercise:rubinstein_non-osdp}
	%
	Consider a variant of the Rubinstein model from \Cref{example:rubinstein} in which in case no proposal is ever accepted, both players' (total, lifetime) payoffs are $1$ rather than $0$.%
		\footnote{`Mutual obstinacy is rewarded in heaven', I guess. It's an artificial assumption.}
	Recall from \Cref{example:rubinstein_osd} the strategy profile $\sigma$.

	\begin{enumerate}[label=(\alph*)]

		\item Show that $\sigma$ is one-shot unimprovable.

		\item Show that $\sigma$ is not subgame-perfect.

		\item Does this variant Rubinstein model satisfy the one-shot deviation principle?
	
	\end{enumerate}
	%
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Continuity at infinity}
\label{osdp:valid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}
	%
	\label{definition:conts_at_infty}
	%
	A deterministic perfect-information extensive-form game
	%
	\begin{equation*}
		(I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
	\end{equation*}
	%
	with terminal histories $Z \subseteq H$ is \emph{continuous at infinity} iff for each player $i \in I$ and every $\eps>0$, there exists a $T \in \N$ such that $\abs*{ u_i(z) - u_i(z') } < \eps$ for any two terminal histories $z,z' \in Z$ which agree in their first $T$ entries.%
		\footnote{That is, $z \equiv (a_1,a_2,a_3,\dots)$ and $z' \equiv (a_1',a_2',a_3',\dots)$ satisfy $a_t=a_t'$ for every $t \in \{1,2,3,\dots,T\}$.}
	%
\end{definition}

In other words, for any small number $\eps>0$, there is a period $T$ late enough that what happens after $T$ cannot affect payoffs by more than $\eps$. The general idea is that the distant future does not loom too large.

\addtocounter{exercise}{-2}
\begin{exercise}[continued]
	%
	\label{exercise:marathon_conts}
	%
	Rigorously prove that the Marathon game is \emph{not} continuous at infinity.
	%
\end{exercise}

\begin{exercise}[continued]
	%
	\label{exercise:rubinstein_non-osdp_cont}
	%
	Rigorously prove that the variant Rubinstein model is \emph{not} continuous at infinity.
	%
\end{exercise}

A word of caution: different authors define `continuity at infinity' differently. The spirit is always the same, namely that the distant future does not loom too large, but the way in which this idea is formalised varies somewhat. The definition above, borrowed from \textcite[section~4.2]{FudenbergTirole1991book}, has the virtue of simplicity. Alternative definitions that I've seen are messier but weaker (hence more widely applicable, at least in principle).

\begin{proposition}
	%
	\label{proposition:osdp_infinite}
	%
	Every deterministic perfect-information extensive-form game that is continuous at infinity satisfies the one-shot deviation principle.
	%
\end{proposition}

The proof relies on a lemma. Recall that $P^{-1}(i) \equiv \{ h \in H \setminus Z : P(h) = i \}$ denotes the set of non-terminal histories at which player $i \in I$ moves.

\begin{lemma}
	%
	\label{lemma:osdp_one_finite}
	%
	Fix a deterministic perfect-information extensive-form game
	%
	\begin{equation*}
		(I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
	\end{equation*}
	%
	with terminal histories denoted $Z \subseteq H$. Further fix a player $i \in I$, a strategy profile $\sigma = (\sigma_i,\sigma_{-i})$, an (alternative) strategy $\sigma_i'$ of player~$i$, and a non-terminal history $h \in P^{-1}(i)$ at which $i$ moves.
	If
	
	\begin{enumerate}[label=(\roman*)]
	
		\item there is a $T \in \N$ such that $\sigma_i(h')=\sigma_i'(h')$ for every $h' \in P^{-1}(i)$ of length $> T$, and

		\item $u_i( h, (\sigma_i',\sigma_{-i})|_h ) > u_i( h, \sigma|_h )$,
	
	\end{enumerate}
	%
	then there is a strategy $\sigma^\star_i$ of player~$i$ and a history $h^\star \in P^{-1}(i)$ at which $i$ moves
	such that

	\begin{enumerate}[label=(\Roman*)]
	
		\item $(\sigma^\star_i,\sigma_{-i})$ is a one-shot deviation from $\sigma$,

		\item $u_i( h^\star, (\sigma_i^\star,\sigma_{-i})|_{h^\star} ) > u_i( h^\star, \sigma|_{h^\star} )$, and

		\item $h^\star$ is a successor of $h$ (that is, $h$ is a truncation of $h^\star$).
	
	\end{enumerate}
	%
\end{lemma}

\Cref{lemma:osdp_one_finite} is a mouthful, but says something quite straightforward: if there is a deviation (from $\sigma_i$ to $\sigma_i'$) that is strictly profitable at some history ($h$) and involves changing actions only in \emph{finitely} many periods (those before $T$), then there is in fact a deviation (from $\sigma_i$ to $\sigma_i^\star$) that is strictly profitable at some history ($h^\star$) and involves changing actions at only a \emph{single} history (namely, that same history $h^\star$). The proof below is almost trivial: we observe since the deviation (from $\sigma_i$ to $\sigma_i'$) makes $i$ strictly better off, there must be a longest history $h^\star$ at which the deviation makes $i$ strictly better off, in which case deviating \emph{only} at that history $h^\star$ obviously makes $i$ strictly better off at that history $h^\star$.

\begin{proof}[Proof of \Cref{lemma:osdp_one_finite}]
	%
	Let $H^\star$ be the set of all non-terminal histories $h' \in H \setminus Z$ such that

	\begin{itemize}
	
		\item $h'$ is a successor of $h$ (i.e. $h$ is a truncation of $h'$),

		\item $i$ moves at $h'$ (i.e. $P(h') = i$), and

		\item $\sigma_i(h') \neq \sigma_i'(h')$.
	
	\end{itemize}
	%
	By hypothesis,
	
	\begin{itemize}
	
		\item the set $H^\star$ is non-empty,

		\item there is a $T \in \N$ such that every $h' \in H^\star$ has length at most $T$ and some $h'' \in H^\star$ has length equal to $T$, and

		\item there are non-terminal histories $h' \in H^\star$ such that
		%
		\begin{equation*}
			u_i\left( h', (\sigma_i',\sigma_{-i})|_{h'} \right)
			> u_i\left( h', \sigma|_{h'} \right) .
		\end{equation*}
		%
		(In particular, this holds for $h' = h \in H^\star$.)
	
	\end{itemize}
	%
	Now, let $h^\star$ be the \emph{longest} history $h' \in H^\star$ at which 
	%
	\begin{equation*}
		u_i( h', (\sigma_i',\sigma_{-i})|_{h'} )
		> u_i( h', \sigma|_{h'} ) ,
	\end{equation*}
	%
	where in case there are multiple such histories of equal maximal length, we choose any one of these arbitrarily. (If that step was too terse/abstract for you, see \Cref{remark:osdp_one_finite} below.)
	Define a strategy $\sigma_i^\star$ of player~$i$ by $\sigma_i^\star(h^\star) \coloneqq \sigma_i'(h^\star)$ and $\sigma_i^\star(h') \coloneqq \sigma_i(h')$ for every $h' \in P^{-1}(i) \setminus \{h^\star\}$.
	%
\end{proof}

\begin{remark}
	%
	\label{remark:osdp_one_finite}
	%
	In the above proof, an equivalent (much longer but perhaps clearer) way of defining the history $h^\star$ is as follows:

	\begin{itemize}
	
		\item let $H^T \subseteq H^\star$ be the longest histories in $H^\star$, i.e. the set of all those $h' \in H^\star$ such that $h'$ is weakly longer than every $h'' \in H^\star$. If there is an $h^T \in H^T$ such that
		%
		\begin{equation*}
			u_i\left( h^T, (\sigma_i',\sigma_{-i})|_{h^T} \right)
			> u_i\left( h^T, \sigma|_{h^T} \right) ,
		\end{equation*}
		%
		then let $h^\star \coloneqq h^T$. If not,

		\item let $H^{T-1} \subseteq H^\star$ be the second-longest histories in $H^\star$, i.e. the set of all those $h' \in H^\star \setminus H^T$ such that $h'$ is weakly longer than every $h'' \in H^\star \setminus H^T$. If there is an $h^{T-1} \in H^{T-1}$ such that
		%
		\begin{equation*}
			u_i\left( h^{T-1}, (\sigma_i',\sigma_{-i})|_{h^{T-1}} \right)
			> u_i\left( h^{T-1}, \sigma|_{h^{T-1}} \right) ,
		\end{equation*}
		%
		then let $h^\star \coloneqq h^{T-1}$. If not,

		\item let $H^{T-2} \subseteq H^\star$ be the third-longest histories in $H^\star$, i.e. the set of all those $h' \in H^\star \setminus \bigl( H^T \cup H^{T-1} \bigr)$ such that $h'$ is weakly longer than every $h'' \in H^\star \setminus \bigl( H^T \cup H^{T-1} \bigr)$. If there is an $h^{T-2} \in H^{T-2}$ such that
		%
		\begin{equation*}
			u_i\left( h^{T-2}, (\sigma_i',\sigma_{-i})|_{h^{T-2}} \right)
			> u_i\left( h^{T-2}, \sigma|_{h^{T-2}} \right) ,
		\end{equation*}
		%
		then let $h^\star \coloneqq h^{T-2}$. If not,

		\item[\vdots]

		\item let $H^{\text{length}(h)+1} \subseteq H^\star$ be the second-shortest histories in $H^\star$, i.e. the set of all those $h' \in H^\star \setminus \{h\}$ such that $h'$ is weakly shorter than every $h'' \in H^\star \setminus \{h\}$. If there is an $h^{\text{length}(h)+1} \in H^{\text{length}(h)+1}$ such that
		%
		\begin{equation*}
			u_i\left( h^{\text{length}(h)+1}, (\sigma_i',\sigma_{-i})|_{h^{\text{length}(h)+1}} \right)
			> u_i\left( h^{\text{length}(h)+1}, \sigma|_{h^{\text{length}(h)+1}} \right) ,
		\end{equation*}
		%
		then let $h^\star \coloneqq h^{\text{length}(h)+1}$. If not,

		\item let $h^\star \coloneqq h$.

	\end{itemize}
	%
\end{remark}

\begin{proof}[Proof of \Cref{proposition:osdp_infinite}]
	%
	Fix a deterministic perfect-information extensive-form game
	%
	\begin{equation*}
		(I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
	\end{equation*}
	%
	that is continuous at infinity, and write $Z \subseteq H$ for its terminal histories. We shall prove the contra-positive: namely, that any strategy profile which fails to be subgame-perfect must also fail to be one-shot unimprovable. So fix a strategy profile $\sigma$ that is not subgame-perfect, meaning that that there is a non-terminal history $h \in H \setminus Z$ and an alternative strategy $\sigma_i'$ of the player $i \coloneqq P(h)$ who moves at $h$ such that $u_i(h,(\sigma'_i,\sigma_{-i})|_h) > u_i(h,\sigma|_h)$; we must show that there exists a \emph{one-shot} deviation $\sigma^\star$ from $\sigma$ such that $u_i(h^\star,(\sigma^\star_i,\sigma_{-i})|_{h^\star}) > u_i(h^\star,\sigma|_{h^\star})$ for some non-terminal history $h^\star \in H \setminus Z$ at which $i$ moves, i.e. $P(h^\star)=i$. (In particular, we shall choose $h^\star$ to be a successor of $h$, i.e. a non-terminal history of which $h$ is a truncation).

	Define
	%
	\begin{equation*}
		\eps \coloneqq u_i(h,(\sigma'_i,\sigma_{-i})|_h) - u_i(h,\sigma|_h) > 0 .
	\end{equation*}
	%
	By continuity at infinity, there is a period $T \in \N$ such that the strategy $\sigma_i^T$ of player~$i$ defined by $\sigma_i^T(h') \coloneqq \sigma_i'(h')$ for histories $h' \in P^{-1}(i)$ of length $\leq T$ and $\sigma_i^T(h') \coloneqq \sigma_i(h')$ for histories $h' \in P^{-1}(i)$ of length $> T$ satisfies
	%
	\begin{equation*}
		u_i\left(h,\left.\left(\sigma^T_i,\sigma_{-i}\right)\right|_h\right) 
		- u_i(h,(\sigma_i',\sigma_{-i})|_h)
		> - \eps .
	\end{equation*}
	%
	and thus
	%
	\begin{multline*}
		u_i\left(h,\left.\left(\sigma^T_i,\sigma_{-i}\right)\right|_h\right)
		- u_i(h,\sigma|_h) 
		\\
		= u_i\left(h,\left.\left(\sigma^T_i,\sigma_{-i}\right)\right|_h\right)
		- u_i(h,(\sigma_i',\sigma_{-i})|_h) 
		+ \eps
		> 0 .
	\end{multline*}
	%
	By construction, $\sigma^T$ differs from $\sigma$ only at histories of length $\leq T$. Hence \Cref{lemma:osdp_one_finite} is applicable, so there exists a one-shot deviation $\sigma^\star$ from $\sigma$ in which $i$ is the deviator and is strictly better off: that is,
	%
	\begin{equation*}
		u_i\left(h^\star,\left.\left(\sigma^T_i,\sigma_{-i}\right)\right|_{h^\star}\right)
		> u_i(h^\star,\sigma|_{h^\star})
		\quad \text{and} \quad
		P(h^\star)=i
	\end{equation*}
	%
	where $h^\star$ is the unique non-terminal history $h' \in H \setminus Z$ such that $\sigma^\star(h') \neq \sigma(h')$.
	%
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sufficient conditions for continuity at infinity}
\label{osdp:suff}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Continuity at infinity is not necessarily easy directly to verify in applications. Fortunately, there are easy-to-check sufficient conditions.

\begin{exercise}
	%
	\label{exercise:conts_infty_finite}
	%
	Show that any deterministic finite-horizon perfect-information extensive-form game
	%
	\begin{equation*}
		(I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
	\end{equation*}
	%
	is continuous at infinity.
	%
\end{exercise}

Another useful sufficient condition is as follows. Whereas in an arbitrary infinite-horizon deterministic perfect-information extensive-form game
%
\begin{equation*}
	(I,\mathcal{A},H,P,A,(u_i)_{i \in I}) ,
\end{equation*}
%
the (total, lifetime) payoff $u_i(z)$ of each player $i \in I$ depends in an arbitrary way on the entire terminal history $(a_1,a_2,a_3,\dots) \equiv z \in Z$, most games in economics have payoffs equal to the discounted sum of \emph{per-period} payoffs: for each player $i \in I$, there is a primitive payoff function $v_i : \mathcal{A} \to \R$ that depends on actions (not on histories, which are \emph{sequences} of actions), and $i$'s payoff at each terminal history $(a_1,a_2,a_3,\dots) \in Z$ satisfies $u_i(a_1,a_2,a_3,\dots) = \sum_{t=1}^\infty \delta_i^{t-1} v_i(a_t)$, where $\delta_i \in (0,1)$ is a parameter called the \emph{discount factor} of player~$i$.

\begin{proposition}
	%
	\label{proposition:conts_infty_disc}
	%
	Fix a deterministic perfect-information extensive-form game
	%
	\begin{equation*}
		\mathcal{G} = (I,\mathcal{A},H,P,A,(u_i)_{i \in I}) 
	\end{equation*}
	%
	with terminal histories $Z \subseteq H$. Suppose that for each player $i \in I$, there exist $v_i : \mathcal{A} \to \R$ and $\delta_i \in (0,1)$ such that $u_i(a_1,a_2,a_3,\dots) = \sum_{t=1}^\infty \delta_i^{t-1} v_i(a_t)$ for every terminal history $(a_1,a_2,a_3,\dots) \in Z$. If for each player $i \in I$, the function $v_i$ is bounded, then $\mathcal{G}$ is continuous at infinity.
	%
\end{proposition}

\begin{proof}
	%
	This could have been an exercise, really, but it would have more of a real-analysis exercise than a game-theory one. Fix a player $i \in I$ and an $\eps > 0$; we seek a $T \in \N$ such that $\abs*{ u_i(a_1,a_2,a_3,\dots) - u_i(a_1',a_2',a_3',\dots) } < \eps$ whenever $a_t = a_t'$ for every $t \in \{1,\dots,T\}$. By hypothesis, there is a constant $K \geq 0$ such that $\abs*{v_i} \leq K$. Choose a $T \in \N$ large enough that $2K \delta_i^T / (1-\delta_i) < \eps$.
	Then for any terminal histories $(a_1,a_2,a_3,\dots) \in Z$ and $(a_1',a_2',a_3',\dots) \in Z$ such that $a_t = a_t'$ for every $t \in \{1,\dots,T\}$, we have
	%
	\begin{multline*}
		\abs*{ u_i(a_1,a_2,a_3,\dots)
		- u_i(a_1',a_2',a_3',\dots) }
		\leq \sum_{t=T+1}^\infty
		\delta_i^{t-1} \abs*{ v_i(a_t) - v_i(a_t') }
		\\
		\leq 2K \sum_{t=T+1}^\infty \delta_i^{t-1}
		= 2K \delta_i^T \sum_{s=0}^\infty \delta_i^s
		= \frac{2K \delta_i^T}{1-\delta_i}
		< \eps ,
	\end{multline*}
	%
	as desired.
	%
\end{proof}

\begin{exercise}
	%
	\label{exercise:rubinstein_final_proof}
	%
	Show that the Rubinstein model (\Cref{example:rubinstein}, \cpageref{example:rubinstein}) is continuous at infinity. (Hint: \Cref{proposition:conts_infty_disc} is \emph{not} applicable [why?], but you can adapt its proof.)
	%
\end{exercise}

\addtocounter{example}{-1}
\begin{example}[continued from \cpageref{example:rubinstein,example:rubinstein_osd}]
	%
	\label{example:rubinstein_final}
	%
	By \Cref{exercise:rubinstein_final_proof}, the Rubinstein model is continuous at infinity, and thus satisfies the one-shot deviation principle. Recall the one-shot unimprovable strategy profile $\sigma$ that we defined. By the one-shot deviation principle, $\sigma$ is subgame-perfect. It can be shown that $\sigma$ is the \emph{only} one-shot unimprovable strategy profile, and thus uniquely subgame-perfect.
	%
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The literature}
\label{osdp:lit}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The one-shot deviation principle for single-agent dynamic decision problems is due to \textcite{Blackwell1965}. Its extension in this chapter to sequentially rational play in multi-player extensive-form games is conceptually only a small step (though a very useful one); I don't know who first took this step, but I suspect it was known as a folk result long before it appeared in print.

For further/alternative reading, consider \textcite[section~4.2]{FudenbergTirole1991book}, \textcite[Exercise~227.1]{OsborneRubinstein1994}, or online lecture notes such as \textcite{Ray2005} or \textcite{Manea2016}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More exercises}
\label{osdp:exer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}
	%
	\label{exercise:joint_venutre}
	%
	Two players $I=\{1,2\}$ players are engaged in a joint venture which yields a gross return of $R > 1/2$ per period for as long as it continues. Each player's stake is initially worth $1/2$. Player~1 (player~2) is in charge of the project in odd (even) periods.

	At the start of period $t \in \mathbf{N}$, the joint venture is worth $R^{t-1}$, so each player's stake is $R^{t-1}/2$. By the end of the period, the joint venture's total worth is $R^t$. At this point, the player in charge can either \emph{continue} with the joint venture, in which case the next period dawns and the other player takes charge, or she can \emph{abscond.} Absconding means permanently ending the joint venture, appropriating the full period-$t$ net return $R^t - R^{t-1}$, and also taking home her own start-of-period stake $R^{t-1}/2$; this leaves the other player with only \emph{her} start-of-period stake, $R^{t-1}/2$.

	Gains are realised only when the joint venture ends (no dividends are paid in the interim), and discounted by $\delta \in (0,1)$. Thus (total, lifetime) payoffs if abscondment occurs in period $t \in \mathbf{N}$ are
	%
	\begin{equation*}
		\left( \delta^{t-1} \left( R^t - R^{t-1}/2 \right), \delta^{t-1} R^{t-1}/2 \right)
		= \left( (R\delta)^{t-1} (R-1/2), (R\delta)^{t-1}/2 \right)
	\end{equation*}
	%
	if $t$ is odd (so player~1 is the absconder) and
	%
	\begin{equation*}
		\left( \delta^{t-1} R^{t-1}/2, \delta^{t-1} \left( R^t - R^{t-1}/2 \right) \right)
		= \left( (R\delta)^{t-1}/2, (R\delta)^{t-1} (R-1/2) \right)
	\end{equation*}
	%
	if $t$ is even (so player~2 is the absconder). In case neither player ever absconds, no gains are ever realised, so both players' payoffs are zero.

	\begin{enumerate}[label=(\alph*)]

		\item Formally, the above is a deterministic perfect-information extensive-form game, meaning a tuple
		%
		\begin{equation*}
			G = (I,\mathcal{A},H,P,A,(u_i)_{i \in I}) ,
		\end{equation*}
		%
		with terminal histories denoted $Z \subseteq H$.
		What are $I$, $\mathcal{A}$, $H \setminus Z$, $Z$, $P$, $A$ and $(u_i)_{i \in I}$?

	\end{enumerate}

	By inspection of the payoffs, time erodes value (so that the sum of players' payoffs is higher the earlier the joint venture is liquidated) if and only if $R\delta<1$. When (and only when) $R\delta \geq 1$, time instead builds value.

	\begin{enumerate}[label=(\alph*),resume]

		\item Prove that if $R\delta<1$, then $G$ is continuous at infinity.

		\item Prove that if $R\delta \geq 1$, then $G$ is \emph{not} continuous at infinity.

	\end{enumerate}

	By inspection of the payoffs, each player has a \emph{(strict) pre-emption motive,} i.e. (strictly) prefers absconding today over having the other player abscond tomorrow, if and only if $(R \delta)^{t-1} ( R - 1/2 ) \geq \mathrel{(>)} (R\delta)^t/2$ for every $t \in \mathbf{N}$, which is equivalent to $R(2-\delta) \geq \mathrel{(>)} 1$. Note that if $R\delta \geq 1$ (time builds value) then $R(2-\delta)>1$ (strict pre-emption motive).

	\begin{enumerate}[label=(\alph*),resume]

		\item Let $\sigma^\text{a}$ be the `always abscond' strategy profile, i.e. $\sigma^\text{a}(h) \coloneqq \text{abscond}$ for every non-terminal history $h \in H \setminus Z$. Prove that if $R(2-\delta) \geq 1$, then $\sigma^\text{a}$ is subgame-perfect.

		\item Prove that if $R(2-\delta) > 1$, then $\sigma^\text{a}$ is the only pure strategy profile that is subgame-perfect.

		\item Bonus: assuming $R\delta > 1$ (which implies $R(2-\delta) > 1$), how many subgame-perfect Nash equilibria do you think there are?

		\item Let $\sigma^\dag$ be the strategy profile whereby player~1 always continues and player~2 always absconds: formally, for each non-terminal history $h \in H \setminus Z$,
		%
		\begin{equation*}
			\sigma^\dag(h) \coloneqq
			\begin{cases}
				\text{continue} & \text{if $\text{length}(h)$ is even} \\
				\text{abscond} & \text{if $\text{length}(h)$ is odd.} 
			\end{cases}
		\end{equation*}
		%
		Prove that if $R(2-\delta) \leq 1$, then $\sigma^\dag$ is subgame-perfect.

	\end{enumerate}
	%
\end{exercise}
